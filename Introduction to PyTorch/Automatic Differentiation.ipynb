{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Automatic Differentiation with `torch.autograd`\n",
    "\n",
    "Deep neural networks enable efficient calculations of gradients through the *back propagation* operation, the computed gradients can thus be used to help the network perform descent steps on the objective (loss) function. With PyTorch's built-in differentiation engine called `torch.autograd`, we can perform automatic computation of the gradients for arbitrary computational graph."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchviz import make_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "x = torch.ones(5) # input tensor\n",
    "y = torch.zeros(3) # target tensor\n",
    "w = torch.randn(5, 3, requires_grad=True) # weights\n",
    "b = torch.randn(3, requires_grad=True) # biases\n",
    "z = torch.matmul(x, w) + b\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y) # compute the cross entropy loss"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.43.0 (0)\n -->\n<!-- Title: %3 Pages: 1 -->\n<svg width=\"342pt\" height=\"577pt\"\n viewBox=\"0.00 0.00 342.00 577.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 573)\">\n<title>%3</title>\n<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-573 338,-573 338,4 -4,4\"/>\n<!-- 139635374666704 -->\n<g id=\"node1\" class=\"node\">\n<title>139635374666704</title>\n<polygon fill=\"#caff70\" stroke=\"black\" points=\"134.5,-30 80.5,-30 80.5,0 134.5,0 134.5,-30\"/>\n<text text-anchor=\"middle\" x=\"107.5\" y=\"-18\" font-family=\"monospace\" font-size=\"10.00\">loss</text>\n<text text-anchor=\"middle\" x=\"107.5\" y=\"-7\" font-family=\"monospace\" font-size=\"10.00\"> ()</text>\n</g>\n<!-- 139632234878128 -->\n<g id=\"node2\" class=\"node\">\n<title>139632234878128</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"299,-151 60,-151 60,-66 299,-66 299,-151\"/>\n<text text-anchor=\"middle\" x=\"179.5\" y=\"-139\" font-family=\"monospace\" font-size=\"10.00\">BinaryCrossEntropyWithLogitsBackward0</text>\n<text text-anchor=\"middle\" x=\"179.5\" y=\"-128\" font-family=\"monospace\" font-size=\"10.00\">&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;</text>\n<text text-anchor=\"middle\" x=\"179.5\" y=\"-117\" font-family=\"monospace\" font-size=\"10.00\">pos_weight: &#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;None</text>\n<text text-anchor=\"middle\" x=\"179.5\" y=\"-106\" font-family=\"monospace\" font-size=\"10.00\">reduction : &#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;1</text>\n<text text-anchor=\"middle\" x=\"179.5\" y=\"-95\" font-family=\"monospace\" font-size=\"10.00\">self &#160;&#160;&#160;&#160;&#160;: [saved tensor]</text>\n<text text-anchor=\"middle\" x=\"179.5\" y=\"-84\" font-family=\"monospace\" font-size=\"10.00\">target &#160;&#160;&#160;: [saved tensor]</text>\n<text text-anchor=\"middle\" x=\"179.5\" y=\"-73\" font-family=\"monospace\" font-size=\"10.00\">weight &#160;&#160;&#160;: &#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;None</text>\n</g>\n<!-- 139632234878128&#45;&gt;139635374666704 -->\n<g id=\"edge11\" class=\"edge\">\n<title>139632234878128&#45;&gt;139635374666704</title>\n<path fill=\"none\" stroke=\"black\" d=\"M146.66,-65.77C139.17,-56.25 131.5,-46.5 124.96,-38.18\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"127.53,-35.8 118.6,-30.1 122.03,-40.13 127.53,-35.8\"/>\n</g>\n<!-- 139635374667984 -->\n<g id=\"node3\" class=\"node\">\n<title>139635374667984</title>\n<polygon fill=\"orange\" stroke=\"black\" points=\"206.5,-30 152.5,-30 152.5,0 206.5,0 206.5,-30\"/>\n<text text-anchor=\"middle\" x=\"179.5\" y=\"-18\" font-family=\"monospace\" font-size=\"10.00\">self</text>\n<text text-anchor=\"middle\" x=\"179.5\" y=\"-7\" font-family=\"monospace\" font-size=\"10.00\"> (3)</text>\n</g>\n<!-- 139632234878128&#45;&gt;139635374667984 -->\n<g id=\"edge1\" class=\"edge\">\n<title>139632234878128&#45;&gt;139635374667984</title>\n<path fill=\"none\" stroke=\"black\" d=\"M179.5,-65.77C179.5,-52.95 179.5,-39.7 179.5,-30.1\"/>\n</g>\n<!-- 139635374360352 -->\n<g id=\"node4\" class=\"node\">\n<title>139635374360352</title>\n<polygon fill=\"orange\" stroke=\"black\" points=\"278.5,-30 224.5,-30 224.5,0 278.5,0 278.5,-30\"/>\n<text text-anchor=\"middle\" x=\"251.5\" y=\"-18\" font-family=\"monospace\" font-size=\"10.00\">target</text>\n<text text-anchor=\"middle\" x=\"251.5\" y=\"-7\" font-family=\"monospace\" font-size=\"10.00\"> (3)</text>\n</g>\n<!-- 139632234878128&#45;&gt;139635374360352 -->\n<g id=\"edge2\" class=\"edge\">\n<title>139632234878128&#45;&gt;139635374360352</title>\n<path fill=\"none\" stroke=\"black\" d=\"M212.34,-65.77C222.43,-52.95 232.85,-39.7 240.4,-30.1\"/>\n</g>\n<!-- 139632234878080 -->\n<g id=\"node5\" class=\"node\">\n<title>139632234878080</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"224,-228 135,-228 135,-187 224,-187 224,-228\"/>\n<text text-anchor=\"middle\" x=\"179.5\" y=\"-216\" font-family=\"monospace\" font-size=\"10.00\">AddBackward0</text>\n<text text-anchor=\"middle\" x=\"179.5\" y=\"-205\" font-family=\"monospace\" font-size=\"10.00\">&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;</text>\n<text text-anchor=\"middle\" x=\"179.5\" y=\"-194\" font-family=\"monospace\" font-size=\"10.00\">alpha: 1</text>\n</g>\n<!-- 139632234878080&#45;&gt;139632234878128 -->\n<g id=\"edge3\" class=\"edge\">\n<title>139632234878080&#45;&gt;139632234878128</title>\n<path fill=\"none\" stroke=\"black\" d=\"M179.5,-186.87C179.5,-179.39 179.5,-170.47 179.5,-161.39\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"183,-161.13 179.5,-151.13 176,-161.13 183,-161.13\"/>\n</g>\n<!-- 139632234882784 -->\n<g id=\"node6\" class=\"node\">\n<title>139632234882784</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"138,-316 13,-316 13,-264 138,-264 138,-316\"/>\n<text text-anchor=\"middle\" x=\"75.5\" y=\"-304\" font-family=\"monospace\" font-size=\"10.00\">SqueezeBackward3</text>\n<text text-anchor=\"middle\" x=\"75.5\" y=\"-293\" font-family=\"monospace\" font-size=\"10.00\">&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;</text>\n<text text-anchor=\"middle\" x=\"75.5\" y=\"-282\" font-family=\"monospace\" font-size=\"10.00\">dim &#160;&#160;&#160;&#160;&#160;&#160;: &#160;&#160;&#160;&#160;&#160;0</text>\n<text text-anchor=\"middle\" x=\"75.5\" y=\"-271\" font-family=\"monospace\" font-size=\"10.00\">self_sizes: (1, 3)</text>\n</g>\n<!-- 139632234882784&#45;&gt;139632234878080 -->\n<g id=\"edge4\" class=\"edge\">\n<title>139632234882784&#45;&gt;139632234878080</title>\n<path fill=\"none\" stroke=\"black\" d=\"M107.84,-263.97C119.95,-254.6 133.73,-243.93 145.95,-234.47\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"148.35,-237.03 154.12,-228.15 144.07,-231.5 148.35,-237.03\"/>\n</g>\n<!-- 139632234884512 -->\n<g id=\"node7\" class=\"node\">\n<title>139632234884512</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"209,-448 0,-448 0,-352 209,-352 209,-448\"/>\n<text text-anchor=\"middle\" x=\"104.5\" y=\"-436\" font-family=\"monospace\" font-size=\"10.00\">MmBackward0</text>\n<text text-anchor=\"middle\" x=\"104.5\" y=\"-425\" font-family=\"monospace\" font-size=\"10.00\">&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;&#45;</text>\n<text text-anchor=\"middle\" x=\"104.5\" y=\"-414\" font-family=\"monospace\" font-size=\"10.00\">mat2 &#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;: &#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;None</text>\n<text text-anchor=\"middle\" x=\"104.5\" y=\"-403\" font-family=\"monospace\" font-size=\"10.00\">mat2_sym_sizes &#160;: &#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;(5, 3)</text>\n<text text-anchor=\"middle\" x=\"104.5\" y=\"-392\" font-family=\"monospace\" font-size=\"10.00\">mat2_sym_strides: &#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;(3, 1)</text>\n<text text-anchor=\"middle\" x=\"104.5\" y=\"-381\" font-family=\"monospace\" font-size=\"10.00\">self &#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;: [saved tensor]</text>\n<text text-anchor=\"middle\" x=\"104.5\" y=\"-370\" font-family=\"monospace\" font-size=\"10.00\">self_sym_sizes &#160;: &#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;(1, 5)</text>\n<text text-anchor=\"middle\" x=\"104.5\" y=\"-359\" font-family=\"monospace\" font-size=\"10.00\">self_sym_strides: &#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;()</text>\n</g>\n<!-- 139632234884512&#45;&gt;139632234882784 -->\n<g id=\"edge5\" class=\"edge\">\n<title>139632234884512&#45;&gt;139632234882784</title>\n<path fill=\"none\" stroke=\"black\" d=\"M91.86,-351.94C89.52,-343.21 87.11,-334.24 84.89,-325.95\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"88.25,-324.99 82.28,-316.24 81.49,-326.81 88.25,-324.99\"/>\n</g>\n<!-- 139632234907824 -->\n<g id=\"node8\" class=\"node\">\n<title>139632234907824</title>\n<polygon fill=\"orange\" stroke=\"black\" points=\"215,-305 156,-305 156,-275 215,-275 215,-305\"/>\n<text text-anchor=\"middle\" x=\"185.5\" y=\"-293\" font-family=\"monospace\" font-size=\"10.00\">self</text>\n<text text-anchor=\"middle\" x=\"185.5\" y=\"-282\" font-family=\"monospace\" font-size=\"10.00\"> (1, 5)</text>\n</g>\n<!-- 139632234884512&#45;&gt;139632234907824 -->\n<g id=\"edge6\" class=\"edge\">\n<title>139632234884512&#45;&gt;139632234907824</title>\n<path fill=\"none\" stroke=\"black\" d=\"M139.79,-351.94C152.41,-335.12 165.72,-317.38 174.7,-305.4\"/>\n</g>\n<!-- 139632234883408 -->\n<g id=\"node9\" class=\"node\">\n<title>139632234883408</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"155,-503 54,-503 54,-484 155,-484 155,-503\"/>\n<text text-anchor=\"middle\" x=\"104.5\" y=\"-491\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n</g>\n<!-- 139632234883408&#45;&gt;139632234884512 -->\n<g id=\"edge7\" class=\"edge\">\n<title>139632234883408&#45;&gt;139632234884512</title>\n<path fill=\"none\" stroke=\"black\" d=\"M104.5,-483.73C104.5,-477.42 104.5,-468.31 104.5,-458.43\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"108,-458.35 104.5,-448.35 101,-458.35 108,-458.35\"/>\n</g>\n<!-- 139635374361472 -->\n<g id=\"node10\" class=\"node\">\n<title>139635374361472</title>\n<polygon fill=\"lightblue\" stroke=\"black\" points=\"134,-569 75,-569 75,-539 134,-539 134,-569\"/>\n<text text-anchor=\"middle\" x=\"104.5\" y=\"-557\" font-family=\"monospace\" font-size=\"10.00\">w</text>\n<text text-anchor=\"middle\" x=\"104.5\" y=\"-546\" font-family=\"monospace\" font-size=\"10.00\"> (5, 3)</text>\n</g>\n<!-- 139635374361472&#45;&gt;139632234883408 -->\n<g id=\"edge8\" class=\"edge\">\n<title>139635374361472&#45;&gt;139632234883408</title>\n<path fill=\"none\" stroke=\"black\" d=\"M104.5,-538.84C104.5,-531.21 104.5,-521.7 104.5,-513.45\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"108,-513.27 104.5,-503.27 101,-513.27 108,-513.27\"/>\n</g>\n<!-- 139632234877936 -->\n<g id=\"node11\" class=\"node\">\n<title>139632234877936</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"334,-299.5 233,-299.5 233,-280.5 334,-280.5 334,-299.5\"/>\n<text text-anchor=\"middle\" x=\"283.5\" y=\"-287.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n</g>\n<!-- 139632234877936&#45;&gt;139632234878080 -->\n<g id=\"edge9\" class=\"edge\">\n<title>139632234877936&#45;&gt;139632234878080</title>\n<path fill=\"none\" stroke=\"black\" d=\"M272.37,-280.39C258.28,-269.48 233.36,-250.19 212.89,-234.34\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"214.77,-231.37 204.72,-228.02 210.48,-236.91 214.77,-231.37\"/>\n</g>\n<!-- 139635374668384 -->\n<g id=\"node12\" class=\"node\">\n<title>139635374668384</title>\n<polygon fill=\"lightblue\" stroke=\"black\" points=\"310.5,-415 256.5,-415 256.5,-385 310.5,-385 310.5,-415\"/>\n<text text-anchor=\"middle\" x=\"283.5\" y=\"-403\" font-family=\"monospace\" font-size=\"10.00\">b</text>\n<text text-anchor=\"middle\" x=\"283.5\" y=\"-392\" font-family=\"monospace\" font-size=\"10.00\"> (3)</text>\n</g>\n<!-- 139635374668384&#45;&gt;139632234877936 -->\n<g id=\"edge10\" class=\"edge\">\n<title>139635374668384&#45;&gt;139632234877936</title>\n<path fill=\"none\" stroke=\"black\" d=\"M283.5,-384.88C283.5,-365.59 283.5,-330.96 283.5,-309.58\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"287,-309.51 283.5,-299.51 280,-309.51 287,-309.51\"/>\n</g>\n</g>\n</svg>\n",
      "text/plain": "<graphviz.graphs.Digraph at 0x7efea9c26770>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_dot(loss, params={\"w\": w, \"b\": b, \"loss\": loss}, show_attrs=True, show_saved=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here `w` and `b` are model parameters we would like to optimize, the computation is made possible by setting `requires_grad = True` when creating those tensors, or later by using `x.requires_grad_(True)`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "A function that we apply to tensors to construct computational graph is an instance of the class [`torch.autograd.Function`](https://pytorch.org/docs/stable/autograd.html#function), which can compute the function in the forward pass as well as the gradients in the backward pass (back propagation)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient function for z = <AddBackward0 object at 0x7efea9c25480>\n",
      "Gradient function for loss = <BinaryCrossEntropyWithLogitsBackward0 object at 0x7efea9c254b0>\n"
     ]
    }
   ],
   "source": [
    "print(f\"Gradient function for z = {z.grad_fn}\")\n",
    "print(f\"Gradient function for loss = {loss.grad_fn}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Gradient Computation\n",
    "\n",
    "By calling `loss.backward()`, we have the gradients computed: `w.grad` = $\\partial loss / \\partial w$ and `b.grad` = $\\partial loss / \\partial b$.\n",
    "\n",
    "<div class=\"alert alert-info\"><h4>Note</h4><p>\n",
    "- We can only obtain `grad` properties for the leaf nodes of the computational graph which have `requires_grad` set to `True`, for other nodes in the computational graph, the gradients are not available.\n",
    "- We can only perform `backward()` once on a given graph for gradient calculations for performance reasons, if we need to perform `backward()` multiple times, we need to set `retain_graph` to `True` in the `backward()` call.</p></div>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0013, 0.3332, 0.2128],\n",
      "        [0.0013, 0.3332, 0.2128],\n",
      "        [0.0013, 0.3332, 0.2128],\n",
      "        [0.0013, 0.3332, 0.2128],\n",
      "        [0.0013, 0.3332, 0.2128]])\n",
      "tensor([0.0013, 0.3332, 0.2128])\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Disable Gradient Tracking\n",
    "\n",
    "Oftentimes we only want to do forward pass (say for inference) and do not need gradient calculations, we can disable gradient tracking by the `torch.no_grad()` block:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "z = torch.matmul(x, w)+b\n",
    "print(z.requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = torch.matmul(x, w)+b\n",
    "print(z.requires_grad)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "or use the `detach()` method on the tensor,"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "z = torch.matmul(x, w)+b\n",
    "z_det = z.detach()\n",
    "print(z_det.requires_grad)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "There are reasons you might want to disable gradient tracking:\n",
    "- To mark some parameters in your neural network as **frozen** parameters.\n",
    "- To **speed up computations** when you are only doing forward pass, because computations on tensors that do not track gradients would be more efficient."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## More on Computational Graphs\n",
    "\n",
    "Conceptually, autograd keeps a record of data (tensors) and all executed operations (along with the resulting new tensors) in a directed acyclic graph (DAG) consisting of [Function](https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function) objects. In this DAG, leaves are the input tensors, roots are the output tensors. By tracing this graph from roots to leaves, you can automatically compute the gradients using the chain rule.\n",
    "\n",
    "In a forward pass, autograd does two things simultaneously:\n",
    "- run the requested operation to compute a resulting tensor\n",
    "- maintain the operation’s *gradient function* in the DAG.\n",
    "\n",
    "The backward pass kicks off when `.backward()` is called on the DAG root. `autograd` then:\n",
    "- computes the gradients from each `.grad_fn`,\n",
    "- accumulates them in the respective tensor’s `.grad` attribute\n",
    "- using the chain rule, propagates all the way to the leaf tensors.\n",
    "\n",
    "<div class=\"alert alert-info\"><h4>Note</h4><p><strong>DAGs are dynamic in PyTorch</strong> An important thing to note is that the graph is recreated from scratch; after each <code>.backward()</code> call, autograd starts populating a new graph. This is exactly what allows you to use control flow statements in your model; you can change the shape, size and operations at every iteration if needed.</p></div>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tensor Gradients and Jacobian Products\n",
    "\n",
    "In many cases, we have a scalar loss function, and we need to compute the gradient with respect to some parameters. However, there are cases when the output function is an arbitrary tensor. In this case, PyTorch allows you to compute so-called **Jacobian product**, and not the actual gradient.\n",
    "\n",
    "For a vector function $\\vec{y}=f(\\vec{x})$, where $\\vec{x}=\\langle x_1,\\dots,x_n\\rangle$ and $\\vec{y}=\\langle y_1,\\dots,y_m\\rangle$, a gradient of $\\vec{y}$ with respect to $\\vec{x}$ is given by the **Jacobian matrix**:\n",
    "\n",
    "\\begin{align}J=\\left(\\begin{array}{ccc}\n",
    "      \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{1}}{\\partial x_{n}}\\\\\n",
    "      \\vdots & \\ddots & \\vdots\\\\\n",
    "      \\frac{\\partial y_{m}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\n",
    "      \\end{array}\\right)\\end{align}\n",
    "\n",
    "Instead of computing the Jacobian matrix itself, PyTorch allows you to compute **Jacobian Product** $v^T\\cdot J$ for a given input vector $v=(v_1 \\dots v_m)$. This is achieved by calling `backward` with $v$ as an argument. The size of $v$ should be the same as the size of the original tensor, with respect to which we want to compute the product:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First call\n",
      "tensor([[4., 2., 2., 2., 2.],\n",
      "        [2., 4., 2., 2., 2.],\n",
      "        [2., 2., 4., 2., 2.],\n",
      "        [2., 2., 2., 4., 2.]])\n",
      "\n",
      "Second call\n",
      "tensor([[8., 4., 4., 4., 4.],\n",
      "        [4., 8., 4., 4., 4.],\n",
      "        [4., 4., 8., 4., 4.],\n",
      "        [4., 4., 4., 8., 4.]])\n",
      "\n",
      "Call after zeroing gradients\n",
      "tensor([[4., 2., 2., 2., 2.],\n",
      "        [2., 4., 2., 2., 2.],\n",
      "        [2., 2., 4., 2., 2.],\n",
      "        [2., 2., 2., 4., 2.]])\n"
     ]
    }
   ],
   "source": [
    "v = torch.eye(4, 5, requires_grad=True) # input\n",
    "out = (v + 1).pow(2).t()\n",
    "out.backward(torch.ones_like(out), retain_graph=True)\n",
    "print(f\"First call\\n{v.grad}\")\n",
    "out.backward(torch.ones_like(out), retain_graph=True)\n",
    "print(f\"\\nSecond call\\n{v.grad}\")\n",
    "v.grad.zero_()\n",
    "out.backward(torch.ones_like(out), retain_graph=True)\n",
    "print(f\"\\nCall after zeroing gradients\\n{v.grad}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Notice that when we call `backward` for the second time with the same argument, the value of the gradient is different. This happens because when doing `backward` propagation, PyTorch **accumulates the gradients**, i.e. the value of computed gradients is added to the `grad` property of all leaf nodes of computational graph. If you want to compute the proper gradients, you need to zero out the `grad` property before. In real-life training an *optimizer* helps us to do this.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<div class=\"alert alert-info\"><h4>Note</h4><p>Previously we were calling <code>backward()</code> function without parameters. This is essentially equivalent to calling <code>backward(torch.tensor(1.0))</code>, which is a useful way to compute the gradients in case of a scalar-valued function, such as loss during neural network training.</p></div>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
